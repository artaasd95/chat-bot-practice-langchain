from typing import Dict, Any, List, Optional, TypedDict, cast
import asyncio
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage
from langchain_core.language_models import BaseLLM
from loguru import logger

from app.services.llm import generate_llm_response


class GraphState(TypedDict, total=False):
    """Type definition for the graph state."""
    messages: List[BaseMessage]
    response: Optional[str]
    metadata: Dict[str, Any]


async def generate_response(state: GraphState, llm: BaseLLM) -> GraphState:
    """Generate a response using the LLM.
    
    Args:
        state: The current state.
        llm: The language model to use.
        
    Returns:
        The updated state with the response.
    """
    messages = state.get("messages", [])
    metadata = state.get("metadata", {})
    
    request_id = metadata.get("request_id", "unknown")
    logger.info(f"Generating response for request ID: {request_id}")
    
    # Get the last message
    if not messages:
        logger.warning(f"No messages to respond to for request ID: {request_id}")
        return {**state, "response": "No messages to respond to."}
    
    try:
        # Generate response using the LLM service
        result = await generate_llm_response(llm, messages)
        response_text = result["response"]
        
        # Add the response to the messages
        updated_messages = messages + [AIMessage(content=response_text)]
        
        logger.info(f"Generated response for request ID: {request_id}")
        logger.debug(f"Response: {response_text}")
        
        # Update the state
        return {
            **state,
            "messages": updated_messages,
            "response": response_text,
            "metadata": {**metadata, **result.get("metadata", {})}
        }
    except Exception as e:
        logger.error(f"Error generating response for request ID {request_id}: {str(e)}")
        raise


async def preprocess_input(state: GraphState) -> GraphState:
    """Preprocess the input messages.
    
    This node can be used for input validation, content moderation,
    or any other preprocessing steps.
    
    Args:
        state: The current state.
        
    Returns:
        The updated state after preprocessing.
    """
    messages = state.get("messages", [])
    metadata = state.get("metadata", {})
    
    request_id = metadata.get("request_id", "unknown")
    logger.info(f"Preprocessing input for request ID: {request_id}")
    
    # Example preprocessing: Truncate very long messages
    MAX_MESSAGE_LENGTH = 4000
    
    processed_messages = []
    for message in messages:
        if len(message.content) > MAX_MESSAGE_LENGTH:
            logger.warning(f"Truncating long message for request ID: {request_id}")
            # Create a new message with truncated content
            truncated_content = message.content[:MAX_MESSAGE_LENGTH] + "... [truncated]"
            processed_message = message.copy()
            processed_message.content = truncated_content
            processed_messages.append(processed_message)
        else:
            processed_messages.append(message)
    
    logger.info(f"Input preprocessing completed for request ID: {request_id}")
    
    return {**state, "messages": processed_messages}


async def postprocess_output(state: GraphState) -> GraphState:
    """Postprocess the output response.
    
    This node can be used for response formatting, content filtering,
    or any other postprocessing steps.
    
    Args:
        state: The current state.
        
    Returns:
        The updated state after postprocessing.
    """
    response = state.get("response", "")
    metadata = state.get("metadata", {})
    
    request_id = metadata.get("request_id", "unknown")
    logger.info(f"Postprocessing output for request ID: {request_id}")
    
    # Example postprocessing: Add a disclaimer to the response
    if response:
        processed_response = response.strip()
        
        # Add a disclaimer if not already present
        disclaimer = "\n\nThis response was generated by an AI assistant."
        if not processed_response.endswith(disclaimer):
            processed_response += disclaimer
        
        logger.info(f"Output postprocessing completed for request ID: {request_id}")
        return {**state, "response": processed_response}
    
    return state